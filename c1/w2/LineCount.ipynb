{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\r\n",
      "drwxr-xr-x   - jovyan supergroup          0 2018-09-22 12:48 wc_mr\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drwxrwxrwx   - jovyan supergroup          0 2017-10-17 13:15 /data/wiki\r\n",
      "drwxrwxrwx   - jovyan supergroup          0 2017-10-17 13:15 /data/wiki/en_articles_part\r\n",
      "-rwxrwxrwx   1 jovyan supergroup   76861985 2017-10-17 13:15 /data/wiki/en_articles_part/articles-part\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls -R /data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only Mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted wc_mr\n",
      "packageJobJar: [/tmp/hadoop-unjar6038274486147558957/] [] /tmp/streamjob1947319111911459340.jar tmpDir=null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18/10/21 16:19:34 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "18/10/21 16:19:34 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "18/10/21 16:19:36 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "18/10/21 16:19:36 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "18/10/21 16:19:37 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1540138670744_0001\n",
      "18/10/21 16:19:38 INFO impl.YarnClientImpl: Submitted application application_1540138670744_0001\n",
      "18/10/21 16:19:38 INFO mapreduce.Job: The url to track the job: http://21ad51e5bf54:8088/proxy/application_1540138670744_0001/\n",
      "18/10/21 16:19:38 INFO mapreduce.Job: Running job: job_1540138670744_0001\n",
      "18/10/21 16:19:53 INFO mapreduce.Job: Job job_1540138670744_0001 running in uber mode : false\n",
      "18/10/21 16:19:53 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "18/10/21 16:20:03 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "18/10/21 16:20:04 INFO mapreduce.Job: Job job_1540138670744_0001 completed successfully\n",
      "18/10/21 16:20:04 INFO mapreduce.Job: Counters: 30\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=275510\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=76874501\n",
      "\t\tHDFS: Number of bytes written=12\n",
      "\t\tHDFS: Number of read operations=10\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=15967\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal time spent by all map tasks (ms)=15967\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=15967\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=16350208\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=4100\n",
      "\t\tMap output records=2\n",
      "\t\tInput split bytes=228\n",
      "\t\tSpilled Records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=313\n",
      "\t\tCPU time spent (ms)=6490\n",
      "\t\tPhysical memory (bytes) snapshot=342568960\n",
      "\t\tVirtual memory (bytes) snapshot=3959042048\n",
      "\t\tTotal committed heap usage (bytes)=286261248\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=76874273\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=12\n",
      "18/10/21 16:20:04 INFO streaming.StreamJob: Output directory: wc_mr\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "OUT_DIR=lc_mr\n",
    "\n",
    "hdfs dfs -rm -r -skipTrash ${OUT_DIR}\n",
    "\n",
    "yarn jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "    -mapper \"wc -l\" \\\n",
    "    -numReduceTasks 0 \\\n",
    "    -input /data/wiki/en_articles_part \\\n",
    "    -output $OUT_DIR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1986\t\r\n",
      "2114\t\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat lc_mr/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducer: Comand line awk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted wc_mr\n",
      "packageJobJar: [/tmp/hadoop-unjar4009347217978947540/] [] /tmp/streamjob2059948752759057123.jar tmpDir=null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18/10/21 16:20:29 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "18/10/21 16:20:30 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "18/10/21 16:20:31 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "18/10/21 16:20:31 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "18/10/21 16:20:31 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1540138670744_0002\n",
      "18/10/21 16:20:32 INFO impl.YarnClientImpl: Submitted application application_1540138670744_0002\n",
      "18/10/21 16:20:32 INFO mapreduce.Job: The url to track the job: http://21ad51e5bf54:8088/proxy/application_1540138670744_0002/\n",
      "18/10/21 16:20:32 INFO mapreduce.Job: Running job: job_1540138670744_0002\n",
      "18/10/21 16:20:43 INFO mapreduce.Job: Job job_1540138670744_0002 running in uber mode : false\n",
      "18/10/21 16:20:43 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "18/10/21 16:20:53 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "18/10/21 16:21:02 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "18/10/21 16:21:03 INFO mapreduce.Job: Job job_1540138670744_0002 completed successfully\n",
      "18/10/21 16:21:03 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=22\n",
      "\t\tFILE: Number of bytes written=414463\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=76874501\n",
      "\t\tHDFS: Number of bytes written=6\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=15180\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=5854\n",
      "\t\tTotal time spent by all map tasks (ms)=15180\n",
      "\t\tTotal time spent by all reduce tasks (ms)=5854\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=15180\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=5854\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=15544320\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=5994496\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=4100\n",
      "\t\tMap output records=2\n",
      "\t\tMap output bytes=12\n",
      "\t\tMap output materialized bytes=28\n",
      "\t\tInput split bytes=228\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=2\n",
      "\t\tReduce shuffle bytes=28\n",
      "\t\tReduce input records=2\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=4\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=322\n",
      "\t\tCPU time spent (ms)=8130\n",
      "\t\tPhysical memory (bytes) snapshot=723808256\n",
      "\t\tVirtual memory (bytes) snapshot=5948452864\n",
      "\t\tTotal committed heap usage (bytes)=551550976\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=76874273\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=6\n",
      "18/10/21 16:21:03 INFO streaming.StreamJob: Output directory: wc_mr\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "OUT_DIR=lc_mr\n",
    "\n",
    "hdfs dfs -rm -r -skipTrash ${OUT_DIR}\n",
    "\n",
    "yarn jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "    -mapper \"wc -l\" \\\n",
    "    -reducer 'awk \"{line_count += $1} END {print line_count}\"' \\\n",
    "    -input /data/wiki/en_articles_part \\\n",
    "    -output $OUT_DIR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4100\t\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat lc_mr/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducer shell file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.sh\n",
    "#!/usr/bin/env sh\n",
    "\n",
    "awk '{line_count += $1} END {print line_count}'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env sh\r\n",
      "\r\n",
      "awk '{line_count += $1} END {print line_count}'\r\n"
     ]
    }
   ],
   "source": [
    "cat reducer.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [/tmp/hadoop-unjar744300748438127911/] [] /tmp/streamjob5011666518483171962.jar tmpDir=null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: `wc_mr': No such file or directory\n",
      "18/10/21 16:33:11 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "18/10/21 16:33:12 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "18/10/21 16:33:13 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "18/10/21 16:33:13 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "18/10/21 16:33:13 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1540138670744_0004\n",
      "18/10/21 16:33:14 INFO impl.YarnClientImpl: Submitted application application_1540138670744_0004\n",
      "18/10/21 16:33:14 INFO mapreduce.Job: The url to track the job: http://21ad51e5bf54:8088/proxy/application_1540138670744_0004/\n",
      "18/10/21 16:33:14 INFO mapreduce.Job: Running job: job_1540138670744_0004\n",
      "18/10/21 16:33:25 INFO mapreduce.Job: Job job_1540138670744_0004 running in uber mode : false\n",
      "18/10/21 16:33:25 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "18/10/21 16:33:35 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "18/10/21 16:33:43 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "18/10/21 16:33:44 INFO mapreduce.Job: Job job_1540138670744_0004 completed successfully\n",
      "18/10/21 16:33:45 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=22\n",
      "\t\tFILE: Number of bytes written=417568\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=76874501\n",
      "\t\tHDFS: Number of bytes written=6\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=15456\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=5287\n",
      "\t\tTotal time spent by all map tasks (ms)=15456\n",
      "\t\tTotal time spent by all reduce tasks (ms)=5287\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=15456\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=5287\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=15826944\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=5413888\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=4100\n",
      "\t\tMap output records=2\n",
      "\t\tMap output bytes=12\n",
      "\t\tMap output materialized bytes=28\n",
      "\t\tInput split bytes=228\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=2\n",
      "\t\tReduce shuffle bytes=28\n",
      "\t\tReduce input records=2\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=4\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=498\n",
      "\t\tCPU time spent (ms)=8410\n",
      "\t\tPhysical memory (bytes) snapshot=734355456\n",
      "\t\tVirtual memory (bytes) snapshot=5941477376\n",
      "\t\tTotal committed heap usage (bytes)=551550976\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=76874273\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=6\n",
      "18/10/21 16:33:45 INFO streaming.StreamJob: Output directory: wc_mr\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "OUT_DIR=wc_mr\n",
    "\n",
    "hdfs dfs -rm -r -skipTrash ${OUT_DIR}\n",
    "\n",
    "yarn jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "    -files reducer.sh \\\n",
    "    -mapper \"wc -l\" \\\n",
    "    -reducer reducer.sh \\\n",
    "    -input /data/wiki/en_articles_part \\\n",
    "    -output $OUT_DIR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4100\t\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat wc_mr/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapper: python script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper_lc.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_lc.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "\n",
    "count = 0\n",
    "for line in sys.stdin:\n",
    "    count += 1\n",
    "\n",
    "print(count)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env python\r\n",
      "\r\n",
      "import sys\r\n",
      "\r\n",
      "count = 0\r\n",
      "for line in sys.stdin:\r\n",
      "    count += 1\r\n",
      "\r\n",
      "print(count)   "
     ]
    }
   ],
   "source": [
    "cat mapper_lc.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted wc_mr\n",
      "packageJobJar: [/tmp/hadoop-unjar8715306462156704944/] [] /tmp/streamjob1520410647142512793.jar tmpDir=null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18/10/21 17:37:18 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "18/10/21 17:37:18 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "18/10/21 17:37:19 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "18/10/21 17:37:20 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "18/10/21 17:37:20 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1540138670744_0008\n",
      "18/10/21 17:37:21 INFO impl.YarnClientImpl: Submitted application application_1540138670744_0008\n",
      "18/10/21 17:37:21 INFO mapreduce.Job: The url to track the job: http://21ad51e5bf54:8088/proxy/application_1540138670744_0008/\n",
      "18/10/21 17:37:21 INFO mapreduce.Job: Running job: job_1540138670744_0008\n",
      "18/10/21 17:37:33 INFO mapreduce.Job: Job job_1540138670744_0008 running in uber mode : false\n",
      "18/10/21 17:37:33 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "18/10/21 17:37:42 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "18/10/21 17:37:50 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "18/10/21 17:37:51 INFO mapreduce.Job: Job job_1540138670744_0008 completed successfully\n",
      "18/10/21 17:37:51 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=22\n",
      "\t\tFILE: Number of bytes written=418468\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=76874501\n",
      "\t\tHDFS: Number of bytes written=6\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=14434\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=5449\n",
      "\t\tTotal time spent by all map tasks (ms)=14434\n",
      "\t\tTotal time spent by all reduce tasks (ms)=5449\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=14434\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=5449\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=14780416\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=5579776\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=4100\n",
      "\t\tMap output records=2\n",
      "\t\tMap output bytes=12\n",
      "\t\tMap output materialized bytes=28\n",
      "\t\tInput split bytes=228\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=2\n",
      "\t\tReduce shuffle bytes=28\n",
      "\t\tReduce input records=2\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=4\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=417\n",
      "\t\tCPU time spent (ms)=7750\n",
      "\t\tPhysical memory (bytes) snapshot=734457856\n",
      "\t\tVirtual memory (bytes) snapshot=5944107008\n",
      "\t\tTotal committed heap usage (bytes)=551026688\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=76874273\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=6\n",
      "18/10/21 17:37:51 INFO streaming.StreamJob: Output directory: wc_mr\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "OUT_DIR=lc_mr\n",
    "\n",
    "hdfs dfs -rm -r -skipTrash ${OUT_DIR}\n",
    "\n",
    "yarn jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "    -files mapper_lc.py,reducer.sh \\\n",
    "    -mapper mapper_lc.py \\\n",
    "    -reducer reducer.sh \\\n",
    "    -numReduceTasks 1 \\\n",
    "    -input /data/wiki/en_articles_part \\\n",
    "    -output $OUT_DIR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4100\t\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat lc_mr/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducer: python script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing reducer_lc.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_lc.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "\n",
    "print(sum([int(value) for value in sys.stdin]))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env python\r\n",
      "\r\n",
      "import sys\r\n",
      "\r\n",
      "print(sum([int(value) for value in sys.stdin]))   "
     ]
    }
   ],
   "source": [
    "cat reducer_lc.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [/tmp/hadoop-unjar1040060431964331637/] [] /tmp/streamjob1402683384088081623.jar tmpDir=null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: `lc_mr': No such file or directory\n",
      "18/10/21 17:47:27 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "18/10/21 17:47:27 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "18/10/21 17:47:29 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "18/10/21 17:47:29 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "18/10/21 17:47:29 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1540138670744_0010\n",
      "18/10/21 17:47:29 INFO impl.YarnClientImpl: Submitted application application_1540138670744_0010\n",
      "18/10/21 17:47:30 INFO mapreduce.Job: The url to track the job: http://21ad51e5bf54:8088/proxy/application_1540138670744_0010/\n",
      "18/10/21 17:47:30 INFO mapreduce.Job: Running job: job_1540138670744_0010\n",
      "18/10/21 17:47:42 INFO mapreduce.Job: Job job_1540138670744_0010 running in uber mode : false\n",
      "18/10/21 17:47:42 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "18/10/21 17:47:52 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "18/10/21 17:48:00 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "18/10/21 17:48:00 INFO mapreduce.Job: Job job_1540138670744_0010 completed successfully\n",
      "18/10/21 17:48:00 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=22\n",
      "\t\tFILE: Number of bytes written=418513\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=76874501\n",
      "\t\tHDFS: Number of bytes written=6\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=16829\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=5626\n",
      "\t\tTotal time spent by all map tasks (ms)=16829\n",
      "\t\tTotal time spent by all reduce tasks (ms)=5626\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=16829\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=5626\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=17232896\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=5761024\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=4100\n",
      "\t\tMap output records=2\n",
      "\t\tMap output bytes=12\n",
      "\t\tMap output materialized bytes=28\n",
      "\t\tInput split bytes=228\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=2\n",
      "\t\tReduce shuffle bytes=28\n",
      "\t\tReduce input records=2\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=4\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=493\n",
      "\t\tCPU time spent (ms)=8280\n",
      "\t\tPhysical memory (bytes) snapshot=723742720\n",
      "\t\tVirtual memory (bytes) snapshot=5943091200\n",
      "\t\tTotal committed heap usage (bytes)=551026688\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=76874273\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=6\n",
      "18/10/21 17:48:00 INFO streaming.StreamJob: Output directory: lc_mr\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "OUT_DIR=lc_mr\n",
    "\n",
    "hdfs dfs -rm -r -skipTrash ${OUT_DIR}\n",
    "\n",
    "yarn jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "    -files mapper_lc.py,reducer_lc.py \\\n",
    "    -mapper mapper_lc.py \\\n",
    "    -reducer reducer_lc.py \\\n",
    "    -numReduceTasks 1 \\\n",
    "    -input /data/wiki/en_articles_part \\\n",
    "    -output $OUT_DIR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4100\t\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat lc_mr/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
